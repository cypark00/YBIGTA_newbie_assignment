# PROMPTING.md

## 1. Prompting 성능 비교

| 프롬프트 방식      | 0-shot 정답률 | 3-shot 정답률 | 5-shot 정답률 |
|:------------------:|:-------------:|:-------------:|:-------------:|
| Direct Prompting   |     0.24      |     0.24      |     0.18      |
| CoT Prompting      |     0.64      |     0.64      |     0.62      |
| 나의 프롬프트 방식 |     0.82      |     0.82      |     0.86      |


- 실험은 GSM8K test split 중 50개 샘플(`num_samples = 50`)을 대상으로 수행됨
- 각 방식별로 0-shot, 3-shot, 5-shot 세팅을 동일하게 적용해 비교
- 나의 프롬프트 방식은 모든 shot 수에서 기존 방법보다 우수한 성능을 기록함


## 2. CoT Prompting이 Direct Prompting보다 뛰어난 이유

Chain of Thought(CoT) 프롬프트는 모델에게 문제를 푸는 "사고 과정"을 유도하는 방식으로, Direct Prompting보다 더 복잡한 문제 해결에 유리합니다.

- Direct Prompting은 정답만을 빠르게 도출하려고 하며, 모델이 문제를 충분히 이해하지 못하고 실수할 가능성이 큽니다.
- 반면, CoT는 모델이 문제를 해결하는 중간 추론 과정을 글로 표현하게 유도함으로써, 인간처럼 단계적으로 사고하게 합니다.
- 특히 GSM8K와 같은 수학 문제에서는 계산 과정의 추적이 중요한데, CoT는 이러한 계산의 정합성을 확보하는 데 도움이 됩니다.

이러한 이유로 CoT는 Direct보다 높은 정답률을 보이며, reasoning이 필요한 문제에서 더 신뢰할 수 있는 성능을 발휘합니다.

---

## 3. 나의 프롬프트 기법이 CoT보다 더 뛰어난 이유

제가 고안한 프롬프트 기법은 CoT(Chain of Thought)의 기본 구조를 유지하면서도, 그 한계를 극복하기 위해 다음과 같은 핵심 전략을 결합한 방식입니다. 이를 통해 reasoning 과정의 안정성과 정답 추출 정확도를 동시에 개선하였으며, 결과적으로 CoT보다 더 높은 정답률을 달성할 수 있었습니다.

### Self-Consistency 전략을 활용한 정답 결정

기존 CoT 프롬프트 방식은 하나의 문제에 대해 단일 응답만을 생성하고 그 결과를 그대로 정답으로 사용하기 때문에, 계산 실수나 문맥 해석 오류 등 **우발적인 오류에 민감**합니다.

이러한 문제를 보완하기 위해 저는 **Self-Consistency 전략**을 도입하여, 하나의 질문에 대해 5개의 응답을 생성하고 그 결과들로부터 *장 일관된 정답 후보를 선택하는 방식을 사용했습니다.

- 구현 방식:
  - 동일한 문제에 대해 **5개의 응답(`num_generations = 5`)을 생성**하고,
  - 각 응답에서 추출한 정답 후보들의 **빈도 수를 집계**한 뒤,
  - **가장 자주 등장한 정답**을 최종 정답으로 선택하며,
  - 가장 자주 등장한 후보 중, **가장 마지막에 등장한 정답을 tie-break 기준**으로 선택

이 방식은 다음과 같은 장점을 가집니다:

- **일관되게 반복 생성된 정답**을 우선시함으로써, 논리적 비약이나 실수가 포함된 응답을 효과적으로 제거
- 다양한 추론 경로를 탐색하고 그 중 다수의 합의에 기반한 정답을 선택하여, 모델의 추론 신뢰도와 안정성을 동시에 확보
- 단일 응답의 정확도에 의존하는 기존 CoT 방식 대비, 집단적 추론(consensus reasoning)의 이점을 활용

결과적으로 Self-Consistency는 모델의 reasoning 다양성을 활용함과 동시에, **가장 신뢰할 수 있는 정답을 선택하는 강건한 결정 메커니즘**으로 작동합니다.

### 정답 추출 로직의 정교화

- Self-Consistency를 활용하기 위해서는, 모델 응답에서 정확하게 정답을 추출하는 과정이 매우 중요합니다. LLM의 응답은 구조가 일정하지 않기 때문에 단순히 마지막 줄을 파싱하는 방식은 불완전합니다. 다양한 reasoning 방식으로 생성된 응답에서는 정답의 위치나 표현 방식이 일정하지 않기 때문에, 단순히 마지막 줄의 숫자만 가져오는 방식은 불안정할 수 있습니다.

이를 보완하기 위해 저는 총 4개의 정규표현식 패턴을 병렬적으로 적용하여, 가능한 모든 표현 방식에서 정답을 추출하도록 설계했습니다:

- `#### [숫자]`: 가장 명시적인 정답 표현
- `Therefore, ...`: 결론 형태로 마무리된 문장 내 숫자 추출
- `Answer:` 또는 `Model response:` 뒤에 오는 숫자
- 그 외 fallback으로 문장 전체에서 가장 마지막에 등장한 숫자

이 방식은 다음과 같은 장점을 가집니다:

- 다양한 포맷과 스타일로 작성된 응답에도 **높은 호환성**을 유지할 수 있음
- 모델 응답의 **다양성과 불확실성에 견고하게 대응**할 수 있음
- 잘못된 파싱으로 인해 정답을 놓치는 확률을 크게 줄일 수 있음

### Temperature 조정을 통한 응답 다양성 확보

- 기존에는 temperature를 `0.3`으로 낮게 설정하여 안정적 응답 생성을 유도했지만, 이는 다양한 추론 경로를 생성하는 데에는 한계가 있었습니다.

이에 따라, 저는 temperature 값을 `0.7`로 조정하여 모델이 **다양한 reasoning 방식과 수식 전개 흐름**을 시도할 수 있도록 유도했습니다. 이는 Self-Consistency 기법과 결합되었을 때 특히 유효했으며:

- 다양한 reasoning 결과가 생성됨으로써 Self-Consistency 투표가 의미 있게 작동
- 평균적으로 더 일관된 정답에 수렴하게 됨
- 단일 방향 사고에 갇히는 것을 피하고, 다양한 추론 루트에 대한 평균적 정답을 선택할 수 있게 됨

### 프롬프트 구조 개선 (Strict Format + Verification 강조)
프롬프트 설계에서 핵심은 **모델이 일관된 reasoning 절차와 검산 과정을 따르도록 구조화하는 것**이었습니다. 이를 위해 다음과 같은 구조적 가이드를 삽입했습니다:

- **INSTRUCTIONS**:
  - 문제를 읽고, 계산을 단계별로 수행하고, 마지막에 반드시 검산하라는 절차 강조
  - `"Only provide final answer after verification"` 같은 강제성 있는 문장 삽입
- **SOLUTION FORMAT**:
  - `"Let me solve this step by step:"` → 단계별 번호 매기기
  - `"Let me verify my answer:"` → 검증 절차 명시
  - 마지막은 `"#### [정답]"`으로 고정하여 정답 추출을 유도

또한, prompt 예시(examples)도 새롭게 직접 제작하여 다음과 같은 기준을 반영했습니다:

- **검산 절차를 실제로 보여주는 구조** 삽입
- 단위 확인, 논리 타당성 검토, 수식 재계산을 포함한 정밀한 검증 과정 예시 추가
- 모델이 일관된 추론 패턴을 학습할 수 있도록 구성

이러한 설계를 통해 모델은 reasoning뿐 아니라 **answer verification**까지 학습하게 되었고, 그 결과 정답률이 크게 향상되었습니다.

### 4. 결론
이러한 세 가지 전략을 조합한 결과, 기존 CoT 방식보다 다음과 같은 성능 향상이 있었습니다:

- reasoning의 정확성 및 다양성 모두 확보
- 정답 추출 과정에서의 안정성 향상
- 0-shot, 3-shot, 5-shot 전 구간에서 CoT 대비 우월한 정답률 달성



